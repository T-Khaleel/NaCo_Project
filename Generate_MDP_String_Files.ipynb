{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from string import ascii_lowercase\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "from unidecode import unidecode #normalize French chars\n",
    "# from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 0.1033955959421126, 'm': 0.024887013798740323, 'a': 0.06536251742872254, '_': 0.17745204096350786, 'b': 0.013379249002355882, 'y': 0.0188783114572816, 'j': 0.001347420549064859, 'n': 0.05683927111880379, 'u': 0.02481609692773691, 's': 0.05109380258666282, 't': 0.07115846915717101, 'v': 0.009311745757007548, 'o': 0.06394418000865426, 'l': 0.033263618443194386, 'i': 0.056019520169238904, 'c': 0.018585028126352227, 'h': 0.0511659214385307, 'p': 0.012670080292321747, 'r': 0.049177845088706186, 'w': 0.01958026828212895, 'd': 0.034355017068128275, 'f': 0.018197990287994615, 'g': 0.016433482378960525, 'x': 0.0016563296312322709, 'q': 0.0010938025866628204, 'k': 0.005725034857445069, 'z': 0.0002103466512813116}\n",
      "dict_keys(['e', 'm', 'a', '_', 'b', 'y', 'j', 'n', 'u', 's', 't', 'v', 'o', 'l', 'i', 'c', 'h', 'p', 'r', 'w', 'd', 'f', 'g', 'x', 'q', 'k', 'z'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/yunfeiliang/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "\n",
    "emma_text = gutenberg.raw('austen-emma.txt')  # Example English text\n",
    "cleaned_emma_text = re.sub(r'[^a-zA-Z ]', '', emma_text)\n",
    "cleaned_emma_text = cleaned_emma_text.lower().replace(' ', '_')\n",
    "# print(cleaned_emma_text)\n",
    "\n",
    "letter_counts = Counter(c.lower() for c in cleaned_emma_text if c.isalpha() or c == \"_\")\n",
    "total = sum(letter_counts.values())\n",
    "english_freq = {char: count/total for char, count in letter_counts.items()}\n",
    "\n",
    "print(english_freq)\n",
    "print(english_freq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 0.07623118238898555, 'h': 0.010647552764959527, 'e': 0.17155798471896513, 'p': 0.026047734321809516, 'r': 0.06241394961797413, 'o': 0.05224298358423481, 'j': 0.0056225886980860885, 'c': 0.031227778198048263, 'g': 0.009295332475981542, 'u': 0.06274113019139117, 'n': 0.06755238671609047, 'b': 0.009909978061880627, 'k': 0.00029502988123156063, 'f': 0.011799304032075044, 'l': 0.058005522354187156, 's': 0.07259815417202511, 'm': 0.02986610182313337, 'i': 0.077016037521749, 'a': 0.08718322112111355, 'y': 0.003907254709130796, 'w': 0.0005730388077766851, 'd': 0.03453929949315379, 'v': 0.019443603903472276, 'q': 0.012552008472653, 'x': 0.003975338527876541, 'z': 0.0018212421514486724, '_': 0.0009342612905666087}\n",
      "how many french keys there are 27\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/yunfeiliang/Documents/EMAI_Courses/Semester_2_RU/Course2_NaCo/Final_Project/les_miserable.txt', 'r', encoding='utf-8') as f:\n",
    "    fr_text = f.read()\n",
    "\n",
    "cleaned_fr_text = re.sub(r'[^a-zA-Z ]', '', fr_text)\n",
    "cleaned_fr_text = fr_text.lower().replace(' ', '_')\n",
    "cleaned_fr_text = unidecode(fr_text)\n",
    "\n",
    "fr_letter_counts = Counter(c.lower() for c in cleaned_fr_text if c.isalpha() or c == \"_\")\n",
    "fr_total = sum(fr_letter_counts.values())\n",
    "french_freq = {char: count/fr_total for char, count in fr_letter_counts.items()}\n",
    "\n",
    "print(french_freq)\n",
    "print(\"how many french keys there are\", len(french_freq.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'm', 'a', '_', 'b', 'y', 'j', 'n', 'u', 's', 't', 'v', 'o', 'l', 'i', 'c', 'h', 'p', 'r', 'w', 'd', 'f', 'g', 'x', 'q', 'k', 'z']\n",
      "27\n",
      "[0.1033955959421126, 0.024887013798740323, 0.06536251742872254, 0.17745204096350786, 0.013379249002355882, 0.0188783114572816, 0.001347420549064859, 0.05683927111880379, 0.02481609692773691, 0.05109380258666282, 0.07115846915717101, 0.009311745757007548, 0.06394418000865426, 0.033263618443194386, 0.056019520169238904, 0.018585028126352227, 0.0511659214385307, 0.012670080292321747, 0.049177845088706186, 0.01958026828212895, 0.034355017068128275, 0.018197990287994615, 0.016433482378960525, 0.0016563296312322709, 0.0010938025866628204, 0.005725034857445069, 0.0002103466512813116]\n"
     ]
    }
   ],
   "source": [
    "# make lists out of both english and french probabilities\n",
    "eng_letters = list(english_freq.keys())\n",
    "eng_probs = list(english_freq.values())\n",
    "fr_letters = list(french_freq.keys())\n",
    "fr_probs = list(french_freq.values())\n",
    "\n",
    "print(eng_letters)\n",
    "print(len(eng_letters))\n",
    "\n",
    "print(eng_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_lines(filepath):\n",
    "#     with open(filepath, 'r') as f:\n",
    "#         return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# # use this to loop through a bunch of stuff\n",
    "# def combine_files(self_file, foreign_file, output_dir=\".\"):\n",
    "#     self_lines = read_lines(self_file)\n",
    "#     foreign_lines = read_lines(foreign_file)\n",
    "    \n",
    "#     #check if code makes sense\n",
    "#     self_name = os.path.splitext(os.path.basename(self_file))[0]\n",
    "#     foreign_name = os.path.splitext(os.path.basename(foreign_file))[0]\n",
    "    \n",
    "#     combined_path = os.path.join(output_dir, f\"TestData_{foreign_name}.txt\")\n",
    "#     combined_labels_path = os.path.join(output_dir, f\"TestData_{foreign_name}.labels\")\n",
    "\n",
    "#     # Combine and shuffle\n",
    "#     combined = list(zip(self_lines + foreign_lines, ['1'] * len(self_lines) + ['0'] * len(foreign_lines)))\n",
    "#     random.shuffle(combined)\n",
    "\n",
    "#     # Unzip after shuffle\n",
    "#     combined_lines, combined_labels = zip(*combined)\n",
    "\n",
    "#     with open(combined_path, 'w') as f:\n",
    "#         for line in combined_lines:\n",
    "#             f.write(line + \"\\n\")\n",
    "        \n",
    "#     with open(combined_labels_path, 'w') as f:\n",
    "#         for label in combined_labels:\n",
    "#             f.write(label + \"\\n\")\n",
    "            \n",
    "#     return combined_path, combined_labels_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./TestData_French_strings_samples=50.txt',\n",
       " './TestData_French_strings_samples=50.labels')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine_files(\"/Users/yunfeiliang/Documents/EMAI_Courses/Semester_2_RU/Course2_NaCo/Final_Project/English_strings_samples=50.txt\", \"/Users/yunfeiliang/Documents/EMAI_Courses/Semester_2_RU/Course2_NaCo/Final_Project/French_strings_samples=50.txt\", output_dir=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27,)\n"
     ]
    }
   ],
   "source": [
    "# alphabet = list(ascii_lowercase)\n",
    "# use the same letters list for all of them, which includes the _\n",
    "alphabet = eng_letters\n",
    "alphabet_idx = {c: i for i, c in enumerate(alphabet)} #create a dictionary where each letter is associated with an index\n",
    "counts = np.zeros((len(alphabet), len(alphabet)), dtype=int)\n",
    "# with open('/Users/yunfeiliang/Documents/EMAI_Courses/Semester_2_RU/Course2_NaCo/Final_Project/war_and_peace.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read().lower()\n",
    "\n",
    "#count all bigrams, use emma instead of war and peace for consistency\n",
    "for c1, c2 in zip(cleaned_emma_text, cleaned_emma_text[1:]):\n",
    "    if c1 in alphabet_idx and c2 in alphabet_idx:\n",
    "        counts[alphabet_idx[c1], alphabet_idx[c2]] += 1\n",
    "\n",
    "row_sums = counts.sum(axis=1, keepdims=True)\n",
    "eng_prob_matrix = counts / row_sums  # prob_matrix[i, j] = P(letter_j | letter_i)\n",
    "\n",
    "# print(\"Rows = previous letter, Columns = next letter\")\n",
    "# print(prob_matrix[:5, :5])  # first 5Ã—5 block as an example\n",
    "\n",
    "print(eng_prob_matrix[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(e | _) = 0.03\n",
      "P(m | _) = 0.06\n",
      "P(a | _) = 0.11\n",
      "P(_ | _) = 0.03\n",
      "P(b | _) = 0.05\n",
      "P(y | _) = 0.02\n",
      "P(j | _) = 0.00\n",
      "P(n | _) = 0.03\n",
      "P(u | _) = 0.01\n",
      "P(s | _) = 0.07\n",
      "P(t | _) = 0.12\n",
      "P(v | _) = 0.01\n",
      "P(o | _) = 0.05\n",
      "P(l | _) = 0.02\n",
      "P(i | _) = 0.07\n",
      "P(c | _) = 0.04\n",
      "P(h | _) = 0.09\n",
      "P(p | _) = 0.03\n",
      "P(r | _) = 0.02\n",
      "P(w | _) = 0.07\n",
      "P(d | _) = 0.03\n",
      "P(f | _) = 0.03\n",
      "P(g | _) = 0.02\n",
      "P(x | _) = 0.00\n",
      "P(q | _) = 0.00\n",
      "P(k | _) = 0.01\n",
      "P(z | _) = 0.00\n"
     ]
    }
   ],
   "source": [
    "# Example, probability of 1 letter coming after the next in English\n",
    "under_idx = alphabet_idx['_']\n",
    "probs_after_underscore = eng_prob_matrix[under_idx]\n",
    "for letter, p in zip(alphabet, probs_after_underscore):\n",
    "    print(f\"P({letter} | _) = {p:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_markov_strings(chars, probs_matrix, dict, n_features=10, n_samples=10):\n",
    "    \"\"\"\n",
    "    Generates a numpy array of random strings.\n",
    "    Args:\n",
    "        chars (list/array): List of potential characters.\n",
    "        probs_matrix (np.array): the probability of one char coming after the next\n",
    "        dict: helps you to figure out which letter is in which index\n",
    "        n_features (int): The length of each string (number of characters).\n",
    "        n_samples (int): The number of strings to generate (number of rows).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D numpy array of strings.\n",
    "    \"\"\"\n",
    "    string_arr = np.zeros((n_samples, n_features), dtype=np.str_)\n",
    "    # Ensure probabilities are a numpy array and normalized to prevent errors\n",
    "    # from tiny floating point inaccuracies if the sum is e.g. 0.9999999999999999\n",
    "    \n",
    "    for i in range(n_samples):        \n",
    "        for j in range(n_features):\n",
    "            if j == 0:\n",
    "                rng = np.random.default_rng()\n",
    "                current_char = rng.choice(chars)\n",
    "                string_arr[i,j] = current_char #assign a purely random char as first char of each string as there were no previous chars\n",
    "                continue\n",
    "            prev_char = string_arr[i,j-1]\n",
    "            prev_idx = dict[prev_char]\n",
    "            # use proabilities associated with the previous char for next char\n",
    "            current_probs = probs_matrix[prev_idx,:]\n",
    "            \n",
    "            if not np.isclose(np.sum(current_probs), 1.0):\n",
    "                print(f\"Warning: Probabilities in generate_random_strings sum to {np.sum(current_probs)}. Normalizing.\")\n",
    "                current_probs /= np.sum(current_probs) #divide by their sum so it's equal to one\n",
    "            \n",
    "            string_arr[i,j] = rng.choice(chars, p=current_probs)\n",
    "    return string_arr\n",
    "\n",
    "def save_numpy_array(arr, name=\"\"):\n",
    "    # input: a numpy array with np.str_ data\n",
    "    np.savetxt(name, arr, fmt='%s', delimiter=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['q' 'u' '_' ... 'e' '_' 'a']\n",
      " ['s' 'a' 'g' ... 'n' 'd' 'n']\n",
      " ['w' 'a' 't' ... '_' 's' 't']\n",
      " ...\n",
      " ['e' 'x' 'i' ... 'e' 'r' 'y']\n",
      " ['n' 'j' 'u' ... 'e' 'd' '_']\n",
      " ['q' 'u' 't' ... 'o' 'l' 'a']]\n"
     ]
    }
   ],
   "source": [
    "sample_size = 124\n",
    "synthetic_eng = generate_markov_strings(alphabet, eng_prob_matrix, dict=alphabet_idx, n_features=10, n_samples=sample_size)\n",
    "print(synthetic_eng)\n",
    "# save_numpy_array(synthetic_eng, f\"English_MDP_Strings2_{sample_size}Samples\")\n",
    "\n",
    "#save it in current directory \n",
    "# save_numpy_array(sample_synthetic_eng, name=\"synthetic_english_MDP_file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#French MDP strings\n",
    "alphabet = eng_letters\n",
    "alphabet_idx = {c: i for i, c in enumerate(alphabet)}\n",
    "counts = np.zeros((len(alphabet), len(alphabet)), dtype=int)\n",
    "with open('/Users/yunfeiliang/Documents/EMAI_Courses/Semester_2_RU/Course2_NaCo/Final_Project/les_miserable.txt', 'r', encoding='utf-8') as f:\n",
    "    fr_text = f.read().lower()\n",
    "    \n",
    "normalized_french = unidecode(fr_text.lower())\n",
    "cleaned_fr_text = normalized_french.lower().replace(' ', '_')\n",
    "\n",
    "#count all bigrams\n",
    "for c1, c2 in zip(cleaned_fr_text, cleaned_fr_text[1:]):\n",
    "    if c1 in alphabet_idx and c2 in alphabet_idx:\n",
    "        counts[alphabet_idx[c1], alphabet_idx[c2]] += 1\n",
    "\n",
    "row_sums = counts.sum(axis=1, keepdims=True)\n",
    "fr_prob_matrix = counts / row_sums  # prob_matrix[i, j] = P(letter_j | letter_i)\n",
    "\n",
    "# check, and all axes add up to roughly 1\n",
    "# print(fr_prob_matrix.shape)\n",
    "# for i in range (len(fr_prob_matrix[0])):\n",
    "#     print(sum(fr_prob_matrix[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['l' 'm' 'b' ... 'h' 'e' '_']\n",
      " ['c' 'o' 'i' ... '_' 'e' '_']\n",
      " ['_' 'm' 'o' ... 'o' 'n' 'e']\n",
      " ...\n",
      " ['d' 'i' 'r' ... 's' 'a' 'b']\n",
      " ['c' 'h' 'o' ... 's' '_' 'c']\n",
      " ['k' '_' 'b' ... 'j' 'e' 's']]\n"
     ]
    }
   ],
   "source": [
    "sample_size = 124\n",
    "synthetic_fr = generate_markov_strings(alphabet, fr_prob_matrix, dict=alphabet_idx, n_features=10, n_samples=sample_size)\n",
    "# save_numpy_array(synthetic_fr, f\"French_MDP_Strings_{sample_size}Samples\")\n",
    "print(synthetic_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['t' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']\n",
      " ['' '' '' '' '' '' '' '' '' '']]\n"
     ]
    }
   ],
   "source": [
    "string_arr_example = np.zeros((10, 10), dtype=np.str_)\n",
    "rng = np.random.default_rng()\n",
    "string_arr_example[0,0] = rng.choice(alphabet)\n",
    "print(string_arr_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept Drift\n",
    "\n",
    "Simulate Concept Drift by progressively making French and English closer\n",
    "\n",
    "Currently the norm between the 2 matrices is 2.0, which means there's a moderate amount of disagreement.\n",
    "\n",
    "We should generate a heat map for it, but we're also using linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9918489683286595\n"
     ]
    }
   ],
   "source": [
    "fr_en_diff = np.linalg.norm(eng_prob_matrix - fr_prob_matrix)\n",
    "print(fr_en_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english probabilities for chars after a:\n",
      "P(e | a ) = 0.00\n",
      "P(m | a ) = 0.02\n",
      "P(a | a ) = 0.00\n",
      "P(_ | a ) = 0.07\n",
      "P(b | a ) = 0.02\n",
      "P(y | a ) = 0.03\n",
      "P(j | a ) = 0.00\n",
      "P(n | a ) = 0.20\n",
      "P(u | a ) = 0.01\n",
      "P(s | a ) = 0.11\n",
      "P(t | a ) = 0.13\n",
      "P(v | a ) = 0.03\n",
      "P(o | a ) = 0.00\n",
      "P(l | a ) = 0.07\n",
      "P(i | a ) = 0.04\n",
      "P(c | a ) = 0.02\n",
      "P(h | a ) = 0.00\n",
      "P(p | a ) = 0.02\n",
      "P(r | a ) = 0.10\n",
      "P(w | a ) = 0.01\n",
      "P(d | a ) = 0.06\n",
      "P(f | a ) = 0.01\n",
      "P(g | a ) = 0.02\n",
      "P(x | a ) = 0.00\n",
      "P(q | a ) = 0.00\n",
      "P(k | a ) = 0.01\n",
      "P(z | a ) = 0.00\n",
      "French probabilities for chars after a:\n",
      "P(e | a ) = 0.00\n",
      "P(m | a ) = 0.02\n",
      "P(a | a ) = 0.00\n",
      "P(_ | a ) = 0.18\n",
      "P(b | a ) = 0.02\n",
      "P(y | a ) = 0.01\n",
      "P(j | a ) = 0.00\n",
      "P(n | a ) = 0.14\n",
      "P(u | a ) = 0.06\n",
      "P(s | a ) = 0.05\n",
      "P(t | a ) = 0.04\n",
      "P(v | a ) = 0.06\n",
      "P(o | a ) = 0.00\n",
      "P(l | a ) = 0.04\n",
      "P(i | a ) = 0.22\n",
      "P(c | a ) = 0.02\n",
      "P(h | a ) = 0.00\n",
      "P(p | a ) = 0.02\n",
      "P(r | a ) = 0.08\n",
      "P(w | a ) = 0.00\n",
      "P(d | a ) = 0.02\n",
      "P(f | a ) = 0.00\n",
      "P(g | a ) = 0.02\n",
      "P(x | a ) = 0.00\n",
      "P(q | a ) = 0.00\n",
      "P(k | a ) = 0.00\n",
      "P(z | a ) = 0.00\n"
     ]
    }
   ],
   "source": [
    "a_idx = alphabet_idx['a']\n",
    "probs_after_a_en = eng_prob_matrix[a_idx]\n",
    "probs_after_a_fr = fr_prob_matrix[a_idx]\n",
    "print(\"english probabilities for chars after a:\")\n",
    "for letter, p in zip(alphabet, probs_after_a_en):\n",
    "    print(f\"P({letter} | a ) = {p:.2f}\")\n",
    "\n",
    "print(\"French probabilities for chars after a:\")\n",
    "for letter, p in zip(alphabet, probs_after_a_fr):\n",
    "    print(f\"P({letter} | a ) = {p:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drifted_matrix(t: float, P1: np.ndarray, P2: np.ndarray) -> np.ndarray:\n",
    "    if not (0 <= t <= 1):\n",
    "        raise ValueError(\"t must be between 0 and 1\")\n",
    "    if P1.shape != P2.shape:\n",
    "        raise ValueError(\"P1 and P2 must have the same shape\")\n",
    "    return (1 - t) * P1 + t * P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['b' 'a' 'd' ... 'o' 'f' 'o']\n",
      " ['p' 'e' 'd' ... '_' 'n' 'c']\n",
      " ['h' 'i' 's' ... 'i' 's' 'e']\n",
      " ...\n",
      " ['l' 'k' 'a' ... 'i' 'e' '_']\n",
      " ['f' 'i' 's' ... 'i' 't' '_']\n",
      " ['y' '_' 'w' ... 'n' '_' 't']]\n"
     ]
    }
   ],
   "source": [
    "#Generate an English file\n",
    "sample_size = 124\n",
    "synthetic_eng = generate_markov_strings(alphabet, eng_prob_matrix, dict=alphabet_idx, n_features=10, n_samples=sample_size)\n",
    "print(synthetic_eng)\n",
    "save_numpy_array(synthetic_eng, f\"string_datasets/negative-selection/MDP_strings/english_training_file/Eng_MDP_Strings.train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[['g' 'e' 'm' ... '_' 's' '_']\n",
      " ['c' 'h' 'e' ... 'd' '_' 's']\n",
      " ['m' 'e' 's' ... 'r' 'i' '_']\n",
      " ...\n",
      " ['e' '_' 'l' ... 'b' 'l' 'e']\n",
      " ['g' 'a' 'u' ... 'v' 'e' 'n']\n",
      " ['y' '_' 'q' ... 'i' 't' 'a']]\n"
     ]
    }
   ],
   "source": [
    "# Generate purely MDP French, unadultered with English\n",
    "drift_percentage = 0\n",
    "drift_decimal = drift_percentage/100\n",
    "print(drift_decimal)\n",
    "\n",
    "# Generate French foreign strings that are injected with 20% of English\n",
    "sample_size = 124\n",
    "synthetic_fr = generate_markov_strings(alphabet, fr_prob_matrix, dict=alphabet_idx, n_features=10, n_samples=sample_size)\n",
    "print(synthetic_fr)\n",
    "save_numpy_array(synthetic_fr, f\"string_datasets/negative-selection/MDP_strings/{drift_percentage}_per_drift/FR_MDP_Strings_Drift{drift_percentage}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "1.5934791746629275\n",
      "[['l' 'a' 'u' ... 'z' '_' 'f']\n",
      " ['y' '_' 's' ... 'r' 'a' '_']\n",
      " ['b' 'u' '_' ... '_' 'a' '_']\n",
      " ...\n",
      " ['e' 'v' 'e' ... 'u' 's' '_']\n",
      " ['t' 'e' 'n' ... 'v' 'a' '_']\n",
      " ['t' '_' 'o' ... 'e' '_' 'l']]\n"
     ]
    }
   ],
   "source": [
    "# make the French probaility matrix progressively more similar to English. in 20% increments.\n",
    "drift_percentage = 20\n",
    "drift_decimal = drift_percentage/100\n",
    "print(drift_decimal)\n",
    "\n",
    "fr_drift_20 = drifted_matrix(0.2, fr_prob_matrix, eng_prob_matrix)\n",
    "fr_en_diff_20 = np.linalg.norm(eng_prob_matrix - fr_drift_20)\n",
    "print(fr_en_diff_20)\n",
    "\n",
    "# Generate French foreign strings that are injected with 20% of English\n",
    "sample_size = 124\n",
    "synthetic_fr = generate_markov_strings(alphabet, fr_drift_20, dict=alphabet_idx, n_features=10, n_samples=sample_size)\n",
    "print(synthetic_fr)\n",
    "save_numpy_array(synthetic_fr, f\"string_datasets/negative-selection/MDP_strings/{drift_percentage}_per_drift/FR_MDP_Strings_Drift{drift_percentage}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "1.1951093809971955\n",
      "[['x' 'e' 'v' ... 's' 't' 'a']\n",
      " ['f' 'a' '_' ... '_' 'u' 'c']\n",
      " ['e' 'n' 't' ... 'n' 'n' 't']\n",
      " ...\n",
      " ['g' '_' 't' ... 'a' 'n' '_']\n",
      " ['t' '_' 'h' ... '_' 'i' 'n']\n",
      " ['q' 'u' 'r' ... 't' 'e' 'l']]\n"
     ]
    }
   ],
   "source": [
    "drift_percentage = 40\n",
    "drift_decimal = drift_percentage/100\n",
    "print(drift_decimal)\n",
    "\n",
    "fr_drift_40 = drifted_matrix(0.4, fr_prob_matrix, eng_prob_matrix)\n",
    "fr_en_diff_40 = np.linalg.norm(eng_prob_matrix - fr_drift_40)\n",
    "print(fr_en_diff_40)\n",
    "\n",
    "sample_size = 124\n",
    "synthetic_fr = generate_markov_strings(alphabet, fr_drift_40, dict=alphabet_idx, n_features=10, n_samples=sample_size)\n",
    "print(synthetic_fr)\n",
    "save_numpy_array(synthetic_fr, f\"string_datasets/negative-selection/MDP_strings/{drift_percentage}_per_drift/FR_MDP_Strings_Drift{drift_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "0.7967395873314637\n",
      "[['c' 'o' 'l' ... 'k' '_' 'c']\n",
      " ['s' 'u' '_' ... '_' 'l' 'a']\n",
      " ['x' '_' 'd' ... 'q' 'u' '_']\n",
      " ...\n",
      " ['p' 'l' 'o' ... 'u' 'e' 'v']\n",
      " ['h' '_' 't' ... 's' 'o' 'n']\n",
      " ['u' 'c' 'u' ... 'a' 's' 'e']]\n"
     ]
    }
   ],
   "source": [
    "drift_percentage = 60\n",
    "drift_decimal = drift_percentage/100\n",
    "print(drift_decimal)\n",
    "\n",
    "fr_drift_60 = drifted_matrix(0.6, fr_prob_matrix, eng_prob_matrix)\n",
    "fr_en_diff_60 = np.linalg.norm(eng_prob_matrix - fr_drift_60)\n",
    "print(fr_en_diff_60)\n",
    "\n",
    "\n",
    "sample_size = 124\n",
    "synthetic_fr = generate_markov_strings(alphabet, fr_drift_60, dict=alphabet_idx, n_features=10, n_samples=sample_size)\n",
    "print(synthetic_fr)\n",
    "save_numpy_array(synthetic_fr, f\"string_datasets/negative-selection/MDP_strings/{drift_percentage}_per_drift/FR_MDP_Strings_Drift{drift_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.3983697936657318\n",
      "[['k' '_' 'b' ... 'n' 'l' 'i']\n",
      " ['a' '_' 'd' ... 'g' 'h' 'e']\n",
      " ['m' '_' 'e' ... 'l' 'a' 'n']\n",
      " ...\n",
      " ['g' 't' '_' ... 'g' 'e' '_']\n",
      " ['j' 'u' 'c' ... 'i' 'n' 'i']\n",
      " ['t' 'e' 'i' ... 's' 'i' 'e']]\n"
     ]
    }
   ],
   "source": [
    "# At 80%, the French is similar, but not indistinguishable from English\n",
    "drift_percentage = 80\n",
    "drift_decimal = drift_percentage/100\n",
    "print(drift_decimal)\n",
    "\n",
    "fr_drift_80 = drifted_matrix(0.8, fr_prob_matrix, eng_prob_matrix)\n",
    "fr_en_diff_80 = np.linalg.norm(eng_prob_matrix - fr_drift_80)\n",
    "print(fr_en_diff_80)\n",
    "\n",
    "sample_size = 124\n",
    "synthetic_fr = generate_markov_strings(alphabet, fr_drift_80, dict=alphabet_idx, n_features=10, n_samples=sample_size)\n",
    "print(synthetic_fr)\n",
    "save_numpy_array(synthetic_fr, f\"string_datasets/negative-selection/MDP_strings/{drift_percentage}_per_drift/FR_MDP_Strings_Drift{drift_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('string_datasets/StringLength=30/samples=1000/TestData_string_0.35_0.35_samples=1000.txt',\n",
       " 'string_datasets/StringLength=30/samples=1000/TestData_string_0.35_0.35_samples=1000.labels')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_val = 1000\n",
    "# combine_files(f\"string_datasets/StringLength=30/samples={x_val}/test_data_self_samples={x_val}.txt\", f\"string_datasets/StringLength=30/samples={x_val}/string_0.20_0.20_samples={x_val}.txt\", output_dir= f'string_datasets/StringLength=30/samples={x_val}')\n",
    "# combine_files(f\"string_datasets/StringLength=30/samples={x_val}/test_data_self_samples={x_val}.txt\", f\"string_datasets/StringLength=30/samples={x_val}/string_0.23_0.23_samples={x_val}.txt\", output_dir= f'string_datasets/StringLength=30/samples={x_val}')\n",
    "# combine_files(f\"string_datasets/StringLength=30/samples={x_val}/test_data_self_samples={x_val}.txt\", f\"string_datasets/StringLength=30/samples={x_val}/string_0.26_0.26_samples={x_val}.txt\", output_dir= f'string_datasets/StringLength=30/samples={x_val}')\n",
    "# combine_files(f\"string_datasets/StringLength=30/samples={x_val}/test_data_self_samples={x_val}.txt\", f\"string_datasets/StringLength=30/samples={x_val}/string_0.29_0.29_samples={x_val}.txt\", output_dir= f'string_datasets/StringLength=30/samples={x_val}')\n",
    "# combine_files(f\"string_datasets/StringLength=30/samples={x_val}/test_data_self_samples={x_val}.txt\", f\"string_datasets/StringLength=30/samples={x_val}/string_0.32_0.32_samples={x_val}.txt\", output_dir= f'string_datasets/StringLength=30/samples={x_val}')\n",
    "# combine_files(f\"string_datasets/StringLength=30/samples={x_val}/test_data_self_samples={x_val}.txt\", f\"string_datasets/StringLength=30/samples={x_val}/string_0.35_0.35_samples={x_val}.txt\", output_dir= f'string_datasets/StringLength=30/samples={x_val}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#Weird Lang, Where transition probabilities are equal\n",
    "N = eng_prob_matrix.shape[1]\n",
    "uniform_matrix = np.ones_like(eng_prob_matrix) / N\n",
    "\n",
    "# probability is the same across all dimensions\n",
    "print(uniform_matrix.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "norm between weirdlang at 0 2.3103675761056004\n",
      "[['k' 'l' 'j' ... 'm' 'k' 'o']\n",
      " ['x' 'j' 'y' ... 'h' 'p' 'm']\n",
      " ['y' 'u' 'z' ... 'a' 'u' 'g']\n",
      " ...\n",
      " ['r' 'n' 'v' ... 'q' 'r' 'o']\n",
      " ['e' '_' 'i' ... 'p' 'a' 'a']\n",
      " ['e' 'w' 'r' ... 'g' 'x' 'z']]\n"
     ]
    }
   ],
   "source": [
    "drift_percentage = 0\n",
    "drift_decimal = drift_percentage/100\n",
    "print(drift_decimal)\n",
    "\n",
    "weirdlang_20 = drifted_matrix(drift_decimal, uniform_matrix, eng_prob_matrix)\n",
    "wl_en_diff_0 = np.linalg.norm(eng_prob_matrix - weirdlang_20)\n",
    "print(f\"norm between weirdlang at {drift_percentage}\", wl_en_diff_0)\n",
    "\n",
    "# Generate weirdlang foreign strings that are injected with 0% of English\n",
    "sample_size = 124\n",
    "weirdlang = generate_markov_strings(alphabet, weirdlang_20, dict=alphabet_idx, n_features=10, n_samples=sample_size)\n",
    "print(weirdlang)\n",
    "save_numpy_array(weirdlang, f\"string_datasets/negative-selection/MDP_strings/weirdlang/{drift_percentage}_per_drift/WL_MDP_Strings_Drift{drift_percentage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "norm between weirdlang at 20 1.8482940608844778\n",
      "[['w' 'n' 'o' ... 'e' 'e' 't']\n",
      " ['t' 't' 'n' ... 'e' 'b' 'e']\n",
      " ['k' '_' 'v' ... 'o' 'i' 's']\n",
      " ...\n",
      " ['k' 'g' '_' ... 'l' 'q' 'e']\n",
      " ['y' '_' 'b' ... 'p' 'u' 'n']\n",
      " ['p' 's' 'p' ... 'l' 'k' 'e']]\n",
      "0.4\n",
      "norm between weirdlang at 40 1.3862205456633583\n",
      "[['k' '_' 'd' ... 'n' 'm' 'q']\n",
      " ['o' 'e' 'e' ... 'z' 'i' 's']\n",
      " ['k' 'e' 'a' ... 'n' 'b' '_']\n",
      " ...\n",
      " ['a' 'c' 'r' ... 's' 'm' 'y']\n",
      " ['g' 'h' 'm' ... 'p' 'e' 'c']\n",
      " ['h' 'n' 'j' ... 'g' '_' 'y']]\n",
      "0.6\n",
      "norm between weirdlang at 60 0.9241470304422389\n",
      "[['j' 'e' 'r' ... 't' 'i' '_']\n",
      " ['l' 'q' 'i' ... '_' 'v' 's']\n",
      " ['r' 'j' 'u' ... 'n' 'e' '_']\n",
      " ...\n",
      " ['z' 'e' 'm' ... '_' 'n' 'r']\n",
      " ['j' 'e' 'l' ... '_' 'a' 'n']\n",
      " ['o' 'w' 'o' ... 'o' 'f' 'e']]\n",
      "0.8\n",
      "norm between weirdlang at 80 0.46207351522111934\n",
      "[['d' 'e' '_' ... 'n' 'g' 'e']\n",
      " ['k' 'z' 'k' ... 'v' 'e' 'd']\n",
      " ['l' 'r' 'u' ... 's' 'h' 'w']\n",
      " ...\n",
      " ['f' 'u' 'r' ... 'y' 's' '_']\n",
      " ['x' 'a' 'r' ... 't' '_' 'r']\n",
      " ['g' '_' 't' ... 'b' 'w' 'a']]\n"
     ]
    }
   ],
   "source": [
    "drift_percentages = [20, 40, 60, 80]\n",
    "drift_percentage = 0\n",
    "\n",
    "for drift_percentage in drift_percentages:\n",
    "    drift_decimal = drift_percentage/100\n",
    "    print(drift_decimal)\n",
    "\n",
    "    weirdlangprobs = drifted_matrix(drift_decimal, uniform_matrix, eng_prob_matrix)\n",
    "    wl_en_diff = np.linalg.norm(eng_prob_matrix - weirdlangprobs)\n",
    "    print(f\"norm between weirdlang at {drift_percentage}\", wl_en_diff)\n",
    "\n",
    "    # Generate weirdlang foreign strings that are injected with 0% of English\n",
    "    sample_size = 124\n",
    "    weirdlang = generate_markov_strings(alphabet, weirdlangprobs, dict=alphabet_idx, n_features=10, n_samples=sample_size)\n",
    "    print(weirdlang)\n",
    "    save_numpy_array(weirdlang, f\"string_datasets/negative-selection/MDP_strings/weirdlang/{drift_percentage}_per_drift/WL_MDP_Strings_Drift{drift_percentage}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
