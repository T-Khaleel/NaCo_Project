{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author Yunfei Liang\n",
    "#Import Statements\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from string import ascii_lowercase\n",
    "from collections import Counter\n",
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "from unidecode import unidecode #for French\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "# from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/yunfeiliang/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "text = gutenberg.raw('austen-emma.txt')  # Example English text\n",
    "letter_counts = Counter(c.lower() for c in text if c.isalpha())\n",
    "total = sum(letter_counts.values())\n",
    "english_freq = {char: count/total for char, count in letter_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 0.1033955959421126, 'm': 0.024887013798740323, 'a': 0.06536251742872254, '_': 0.17745204096350786, 'b': 0.013379249002355882, 'y': 0.0188783114572816, 'j': 0.001347420549064859, 'n': 0.05683927111880379, 'u': 0.02481609692773691, 's': 0.05109380258666282, 't': 0.07115846915717101, 'v': 0.009311745757007548, 'o': 0.06394418000865426, 'l': 0.033263618443194386, 'i': 0.056019520169238904, 'c': 0.018585028126352227, 'h': 0.0511659214385307, 'p': 0.012670080292321747, 'r': 0.049177845088706186, 'w': 0.01958026828212895, 'd': 0.034355017068128275, 'f': 0.018197990287994615, 'g': 0.016433482378960525, 'x': 0.0016563296312322709, 'q': 0.0010938025866628204, 'k': 0.005725034857445069, 'z': 0.0002103466512813116}\n"
     ]
    }
   ],
   "source": [
    "#clean text and generate random sampels\n",
    "cleaned_eng_text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "cleaned_eng_text = cleaned_eng_text.lower().replace(' ', '_')\n",
    "\n",
    "def Draw_Samples_From_Text(n_features, n_samples, cleaned_text, lang='e'):\n",
    "    text_samples = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        start = random.randint(0, len(cleaned_text) - n_features)\n",
    "        sample = cleaned_text[start:start + n_features]\n",
    "        text_samples.append(sample)\n",
    "    \n",
    "    if lang == 'e':\n",
    "        language = 'English'\n",
    "    else:\n",
    "        language = 'French'\n",
    "    \n",
    "    with open(f'{language}_{n_samples}_samples.txt', 'w') as f:\n",
    "        for line in text_samples:\n",
    "            f.write(line + '\\n')\n",
    "    print(\"file saved in : \", os.getcwd())\n",
    "    \n",
    "    return text_samples\n",
    "\n",
    "letter_counts = Counter(ch.lower() for ch in cleaned_eng_text if ch.isalpha() or ch == \"_\")\n",
    "total = sum(letter_counts.values())\n",
    "english_freq = {char: count/total for char, count in letter_counts.items()}\n",
    "print(english_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved in :  /Users/yunfeiliang/Documents/EMAI_Courses/Semester_2_RU/Course2_NaCo/Final_Project\n",
      "['_last_june', 'e_on_whom_', 'ly_cast_do', '_leaving_h', 'onger_time', '_repressed', 'tion_was_m', 'lighted_ye', 'he_true_ha', 'he_had_gon', 'f_a_cold_s', '_at_the_cr', 'uch_a_perf', '_sat_toget', 'aces_shewe', 'wo_lines_a', 'rcely_fini', 'ets_going_', '_superior_', 'd_you_had_', 'and_the_ho', 'tonhas_don', '_societyit', 'ggle_than_', 'alf_the_di', 'mself_her_', 'ir_daughte', '_hartfield', 'indquarter', '_an_hour_a', 'to_emma_an', 'her_pale_b', 'cured_for_', 'dow_of_aut', 'the_proces', 'ng_a_part_', 'ousei_am_n', 'ans__we_sh', 'elldoing_c', 'was_natura', 'ineof_his_', 'd_strength', '_the_limew', 'ed_her_on_', 'dent_or_a_', 'd_or_in_he', 'ood_lady_h', 'u_might_wi', '_thing_els', 'ton_must_a', 'n_she_thou', 'w_that_the', 'll_done_of', '_of_thever', 'came_to_hi', 'at_all_ple', '_while_she', 'emn_nonsen', 'nd_thenas_', 'uarrel_wit', 'llections_', 'quencenoth', 'ad_wreath_', 'ing_quite_', 'ies_of_som', 'some_time_', 'r_of_suffe', 'rnsand_eve', 'r_and_ever', 'd_particul', 'ngement_wh', 'g_that_he_', 'idof_your_', '_a_farmer_', 'ed_them_bu', 'ealmentsuc', 'axand_ther', 'atural_you', 'oo_to_take', 'gup_her_ha', '_such_a_ju', '_of_the_na', 'le_danger_', 'ou_do_such', 'ng__he_sta', 'to_feel_th', 'e__his_com', 'ed_to_give', 'engagement', 'entfrom_he', 'n_to_think', 'ur_vain_sp', 'this_suffe', 'nce_and_my', 'use_they_m', 'k_in_her_h', 'edge_of_ci', 'at_theeffe', 'was_quite_', 'een_so_tho', '_saidthat_', 'y_last_and', 'miss_woodh', 'two_moment', 'ling_the_h', '_least_tha', '_sure_it_a', 'ondemn_him', 'existenceo', 'as_to_be_t', 'ed__thank_', '_you_amuse', 's_too_well', 'had_been_e', 'o_beacquai', 'ven_wilful', '_he_will_b', 't_good_pla', 'ers__i_hop', 'n_me_to_be', 'e_wasa_lit', 'mth_of_sen', 'the_genera', 'emain_till']\n"
     ]
    }
   ],
   "source": [
    "eng_text_sample = Draw_Samples_From_Text(n_features=10,n_samples=124,cleaned_text=cleaned_eng_text, lang='e')\n",
    "print(eng_text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved in :  /Users/yunfeiliang/Documents/EMAI_Courses/Semester_2_RU/Course2_NaCo/Final_Project\n",
      "['ain_tout_e', 'l_seleva_d', '_de_nouvea', 'medecin_el', 'president_', 'ait_tres_f', 't_lenvoya_', 'spredicati', 'l_avait_to', 'es_arbores', 's_du_cimet', 'ssous_mon_', 's_dun_nomm', 'que_diable', 'rite_silun', 'rde_ses_ch', 'uun_eveque', '_sur_des_f', 'rmes_et_de', '_un_moment', 'ari_la_bat', 'te_estouve', 'nt_les_pla', '_quil_est_', 'e_doucefai', '_enfant_su', 'ez_bombard', 'nt_ce_sena', 'a_des_deux', 'receipt_of', '_se_mit_a_', 'st_guere_q', '_quen_sort', '_cette_fol', 'tes_gens_e', '_conduisai', 'etits_drol', 'tiles_puis', 'iers_eusse', 't_charmant', 'ie_luniver', 'ns_preuves', 'syche_de_v', 'mes_plus_q', 'que_je_nai', '_retenu_et', 's_ses_memb', 'tousles_jo', 're_fit_pla', '_jai_pu_ap', '_bonne_tab', 'nvers_luit', 'oitedisaie', '_seraitsa_', 'stribute_o', 'e_par_la_t', 'nde_fille_', 'es_dimanch', '_mauvais_c', 'sieur_dema', 'eur_navait', 'doc_onte_a', 'venu_nous_', 'la_marque_', '_grattanta', '_pese_comm', 'les_mains_', 'les_et_ils', 'un_bapteme', '_de_napole', 'scene_tell', '_de_chambr', 'lons_baton', 'ntirait_qu', 'ard_qui_ve', 'aient_sa_t', 'onnet_vert', 'nt_transmi', 'teme_acote', 'e_et_chari', 'le_silence', 'r_la_malad', 'infirmerie', 'au_carcan_', 'les_manger', 'ate_your_a', 'ntinua_sa_', '_allait_st', 's_yeux_fix', 'curieuse_e', 'us_les_ret', 'e_plus_lon', 'ujours_le_', 'et_fantine', 'aucoupde_p', 'rite_quoiq', 'ornons_a_d', '_mon_passe', 't_pas_plus', '_pour_rito', 'moi_interr', 'phelins_ci', 'sous_en_ci', 'ses_presen', 'le_les_adm', 'aute_ils_n', 'ecommencee', 'tout_netai', 'un_mot_la_', 'us_les_bie', '_braconnie', 'expres_pou', 'tte_langue', 'nous_achet', 'mmeasoldat', 'ous_le_poi', 'e_de_vetem', 'rale_il_fi', 'positions_', 'ort_royali', '_tete_embr', 'cela_voila', 'nt_seuleme', 'vait_jean_']\n"
     ]
    }
   ],
   "source": [
    "#same with French\n",
    "fr_text_sample = Draw_Samples_From_Text(n_features=10,n_samples=124,cleaned_text=cleaned_fr_text, lang='f')\n",
    "print(fr_text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 0.06387074999168102, 'h': 0.008921115472193215, 'e': 0.1437408788328709, '_': 0.16292682308461431, 'p': 0.021824249271495056, 'r': 0.052293899256362436, 'o': 0.04377209481418568, 'j': 0.004710919413646613, 'c': 0.02616437987155495, 'g': 0.007788149652900472, 'u': 0.052568029447603894, 'n': 0.056599169369674865, 'b': 0.008303134116215355, 'k': 0.0002471925423911442, 'f': 0.009886117128066337, 'l': 0.04860027222871015, 's': 0.0608267956715952, 'm': 0.025023491214365055, 'i': 0.06452834553714464, 'a': 0.07304698084416253, 'y': 0.00327371661910323, 'w': 0.00048012397656741466, 'd': 0.0289389577031376, 'v': 0.01629093928412406, 'q': 0.01051677502467964, 'x': 0.0033307610519627247, 'z': 0.0015259385789914862}\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/yunfeiliang/Documents/EMAI_Courses/Semester_2_RU/Course2_NaCo/Final_Project/les_miserable.txt', 'r', encoding='utf-8') as f:\n",
    "    fr_text = f.read()\n",
    "\n",
    "normalized_fr_text = unidecode(fr_text.lower())\n",
    "cleaned_fr_text = re.sub(r'[^a-zA-Z ]', '', normalized_fr_text)\n",
    "cleaned_fr_text = cleaned_fr_text.lower().replace(' ', '_')\n",
    "\n",
    "fr_letter_counts = Counter(c.lower() for c in cleaned_fr_text if c.isalpha() or c == \"_\")\n",
    "fr_total = sum(fr_letter_counts.values())\n",
    "french_freq = {char: count/fr_total for char, count in fr_letter_counts.items()}\n",
    "\n",
    "print(french_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('e', 0.1033955959421126), ('m', 0.024887013798740323), ('a', 0.06536251742872254), ('_', 0.17745204096350786), ('b', 0.013379249002355882), ('y', 0.0188783114572816), ('j', 0.001347420549064859), ('n', 0.05683927111880379), ('u', 0.02481609692773691), ('s', 0.05109380258666282), ('t', 0.07115846915717101), ('v', 0.009311745757007548), ('o', 0.06394418000865426), ('l', 0.033263618443194386), ('i', 0.056019520169238904), ('c', 0.018585028126352227), ('h', 0.0511659214385307), ('p', 0.012670080292321747), ('r', 0.049177845088706186), ('w', 0.01958026828212895), ('d', 0.034355017068128275), ('f', 0.018197990287994615), ('g', 0.016433482378960525), ('x', 0.0016563296312322709), ('q', 0.0010938025866628204), ('k', 0.005725034857445069), ('z', 0.0002103466512813116)])\n",
      "27\n",
      "dict_items([('t', 0.06387074999168102), ('h', 0.008921115472193215), ('e', 0.1437408788328709), ('_', 0.16292682308461431), ('p', 0.021824249271495056), ('r', 0.052293899256362436), ('o', 0.04377209481418568), ('j', 0.004710919413646613), ('c', 0.02616437987155495), ('g', 0.007788149652900472), ('u', 0.052568029447603894), ('n', 0.056599169369674865), ('b', 0.008303134116215355), ('k', 0.0002471925423911442), ('f', 0.009886117128066337), ('l', 0.04860027222871015), ('s', 0.0608267956715952), ('m', 0.025023491214365055), ('i', 0.06452834553714464), ('a', 0.07304698084416253), ('y', 0.00327371661910323), ('w', 0.00048012397656741466), ('d', 0.0289389577031376), ('v', 0.01629093928412406), ('q', 0.01051677502467964), ('x', 0.0033307610519627247), ('z', 0.0015259385789914862)])\n",
      "27\n",
      "{'e': -0.3294471427296243, 'm': -0.005468679871644179, 'a': -0.11115223174507032, '_': 0.08539871638315723, 'b': 0.47702618747870656, 'y': 1.7518353538033757, 'j': -1.2511613999831108, 'n': 0.004233095475102332, 'u': -0.7505943446680369, 's': -0.17436406875102003, 't': 0.10804623644823554, 'v': -0.5592864896780868, 'o': 0.3790067989454146, 'l': -0.3791553851993466, 'i': -0.14140203469354776, 'c': -0.3420270355852537, 'h': 1.746560178567842, 'p': -0.543745268538404, 'r': -0.06143528459617744, 'w': 3.706203703918939, 'd': 0.17155391303991407, 'f': 0.6101335043432612, 'g': 0.7466500164584311, 'x': -0.6982933574285561, 'q': -2.262492624522925, 'k': 3.1385737476659825, 'z': -1.9775203986255827}\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6\n",
    "\n",
    "print(english_freq.items())\n",
    "print(len(english_freq.keys()))\n",
    "print(french_freq.items())\n",
    "print(len(french_freq.keys()))\n",
    "\n",
    "diff = {\n",
    "    k: math.log((english_freq[k] + eps ) / (french_freq[k] + eps)) \n",
    "        for k in english_freq\n",
    "        }\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_strings(chars, probs, n_features, n_samples):\n",
    "    \"\"\"\n",
    "    Generates a numpy array of random strings.\n",
    "    Args:\n",
    "        chars (list): List of potential characters.\n",
    "        probs (list or np.array): Probabilities corresponding to chars. Must sum to 1.\n",
    "        n_features (int): The length of each string (number of characters).\n",
    "        n_samples (int): The number of strings to generate (number of rows).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D numpy array of strings.\n",
    "    \"\"\"\n",
    "    # string_arr = np.zeros((n_samples, n_features), dtype=np.str_)\n",
    "    string_arr = np.zeros((n_samples, n_features), dtype='U50')\n",
    "\n",
    "    # Ensure probabilities are a numpy array and normalized to prevent errors\n",
    "    # from tiny floating point inaccuracies if the sum is e.g. 0.9999999999999999\n",
    "    current_probs = np.array(probs, dtype=float)\n",
    "    if not np.isclose(np.sum(current_probs), 1.0):\n",
    "        print(f\"Warning: Probabilities in generate_random_strings sum to {np.sum(current_probs)}. Normalizing.\")\n",
    "        current_probs /= np.sum(current_probs) #divide by their sum so it's equal to one\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        random_chars = np.random.choice(chars, size=n_features, p=current_probs)\n",
    "        string_arr[i,:] = random_chars\n",
    "    \n",
    "    #remove \\n\n",
    "    string_arr_clean = np.char.replace(string_arr, '\\n', ' ')\n",
    "    \n",
    "    return string_arr_clean\n",
    "    \n",
    "\n",
    "def save_numpy_array(arr, name=\"\"):\n",
    "    # input: a numpy array with np.str_ data\n",
    "    np.savetxt(name, arr, fmt='%s', delimiter=\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_concept_drift(n_features=10, n_samples=100, chars = ['a','b','c'], probs = [0.1,0.8,0.1], drift_chars = ['a'], drift_iters = 5, drift_percent = 0.1, language = 'English'):\n",
    "#     \"\"\"\n",
    "#     Creates a new file in the local directory for each drift iteration\n",
    "#     n_features = how many chars per string\n",
    "#     n_samples = how many strings per file\n",
    "#     chars = all potential chars the strings will use\n",
    "#     probs = the distribution probability each letter will use. independent of each other\n",
    "#     drift_chars = the characters we're modifying the probabilities of\n",
    "#     drift_iters = how many times we'll decrease the probabilities of the drift chars\n",
    "#     drift_percent = how much the chosen chars will decrease their % by\n",
    "#     \"\"\"\n",
    "#     # in this iteration, it assumes we only have 3 potential cars\n",
    "#     # so all it modifies is the probability of the second char\n",
    "#     # increases probability of char 1 up to max delta, and decreases probability of the second one\n",
    "#     if (len(chars) != len(probs)):\n",
    "#         raise ValueError(\"All chars must have a probability\")\n",
    "    \n",
    "#     original_probs = pd.Series(data=probs, index=chars)\n",
    "#     drift_per_char = drift_percent/len(drift_chars)\n",
    "#     modified_probs = original_probs\n",
    "#     all_other_chars = list(set(chars) - set(drift_chars)) #find out which chars to exclude\n",
    "#     driftpercent_other_chars = drift_percent / len(all_other_chars) #the percentage all other chars should decrease by, per iter\n",
    "    \n",
    "#     for _ in range(drift_iters):\n",
    "        \n",
    "#         # check if all probabilities are still roughly 1.0\n",
    "#         if (np.isclose(modified_probs.sum(),1.0)):\n",
    "#             print(\"probabilities add up to 1\")\n",
    "#         else:\n",
    "#             print(\"probabilities did not add up to 1, normalizing\")\n",
    "#             modified_probs = modified_probs/modified_probs.sum()\n",
    "        \n",
    "#         probs_array = modified_probs.values # get all probabilities as numpy array, if pandas does it automatically\n",
    "#         arr = generate_random_strings(chars, probs_array, n_features, n_samples) #body of the text\n",
    "        \n",
    "#         drift_probs_array = modified_probs[drift_chars] #turn the probabitilies of the drift chars into an array\n",
    "#         print(f\"characters modified: {drift_chars}, probabilities: {drift_probs_array}\")\n",
    "\n",
    "#         probs_name_string = '_'.join(f\"{x:.2f}\" for x in drift_probs_array) #turn the probabilities into a string separated by _\n",
    "#         # commented out to not list a huge probability string\n",
    "#         # np.savetxt(f'string_{probs_name_string}_samples={n_samples}.txt', arr, fmt='%s', delimiter=\"\")\n",
    "#         np.savetxt(f'{language}_strings_samples={n_samples}.txt', arr, fmt='%s', delimiter=\"\")\n",
    "        \n",
    "#         # modify the probabilities for the next iter. Subtract probabilities from the chosen letters, add to all others to balance it all out\n",
    "#         modified_probs[drift_chars] -= drift_per_char\n",
    "#         modified_probs[all_other_chars] += driftpercent_other_chars\n",
    "        \n",
    "#         # check if the decrement made any of the chars have negative percentage\n",
    "#         if ((modified_probs < 0).any()):\n",
    "#             raise ValueError(\"Too many iterations! Some of the chars now have negative probabilities!\")\n",
    "        \n",
    "\n",
    "# # Legacy code for reference\n",
    "# # while (local_delta < max_delta):\n",
    "# #     arr = generate_random_strings(chars, modified_probs, n, x)\n",
    "# #     # local probabilities are adjusted for each iteration.\n",
    "    \n",
    "# #     #change the probability of the 1st and second chars. Have to make dynamic if there are more\n",
    "# #     # increment after each loop\n",
    "# #     local_delta += delta\n",
    "    \n",
    "# #     probs_name_string = '_'.join(f\"{x:.1f}\" for x in modified_probs)\n",
    "    \n",
    "# #     # save the generated strings during each loop iteration in a .txt file\n",
    "# #     np.savetxt(f'string_{probs_name_string}_x={x}.txt', arr, fmt='%s', delimiter=\"\")\n",
    "# #     modified_probs[0] += delta\n",
    "# #     modified_probs[1] -= delta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = list(english_freq.keys())\n",
    "english_probs = list(english_freq.values())\n",
    "fr_letters = list(french_freq.keys())\n",
    "fr_probs = list(french_freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_text_underscore = generate_random_strings(letters, english_probs, 10, 124)\n",
    "save_numpy_array(naive_text_underscore, name=\"Naive_English_strings_124samples_underscore.train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = 124\n",
    "naive_text_underscore = generate_random_strings(letters, english_probs, 10, 124)\n",
    "save_numpy_array(naive_text_underscore, name=f\"Naive_English_strings_{lines}samples_underscore.test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 124\n",
    "naive_text = generate_random_strings(fr_letters, fr_probs, 10, 124)\n",
    "save_numpy_array(naive_text, name=f\"Naive_French_strings_{samples}samples.test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.037037037037037035\n",
      "1.0\n",
      "{'e': 0.037037037037037035, 'm': 0.037037037037037035, 'a': 0.037037037037037035, '_': 0.037037037037037035, 'b': 0.037037037037037035, 'y': 0.037037037037037035, 'j': 0.037037037037037035, 'n': 0.037037037037037035, 'u': 0.037037037037037035, 's': 0.037037037037037035, 't': 0.037037037037037035, 'v': 0.037037037037037035, 'o': 0.037037037037037035, 'l': 0.037037037037037035, 'i': 0.037037037037037035, 'c': 0.037037037037037035, 'h': 0.037037037037037035, 'p': 0.037037037037037035, 'r': 0.037037037037037035, 'w': 0.037037037037037035, 'd': 0.037037037037037035, 'f': 0.037037037037037035, 'g': 0.037037037037037035, 'x': 0.037037037037037035, 'q': 0.037037037037037035, 'k': 0.037037037037037035, 'z': 0.037037037037037035}\n"
     ]
    }
   ],
   "source": [
    "# Generate a dictionary where all distributions are the same. a nonsense language\n",
    "uniform_dict = english_freq.copy()\n",
    "num_chars = len(uniform_dict)\n",
    "uniform_prob = 1/num_chars\n",
    "print(uniform_prob)\n",
    "for char in uniform_dict:\n",
    "    uniform_dict[char] = uniform_prob\n",
    "\n",
    "print(sum(uniform_dict.values()))\n",
    "print(uniform_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(uniform_dict.keys())\n",
    "uniform_probs_list = list(uniform_dict.values())\n",
    "weird_naive_text = generate_random_strings(chars, uniform_probs_list, 10, 124)\n",
    "save_numpy_array(weird_naive_text, name=\"naive_weirdlang_strings_124samples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERFORMS LINEAR INTERPOLATION BETWEEN 2 DICTIONARIES t is a value between 1 and 0, and it's how much I reduce the influence of\n",
    "#P1 by so it steadily becomes 20% closer to P2\n",
    "def drifted_dict(t, P1, P2):\n",
    "    # P1, P2: dicts symbol → prob, with the same key‐set\n",
    "    return {\n",
    "        k: (1 - t) * P1[k] + t * P2[k]\n",
    "        for k in P1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 0.050308748818052154, 'm': 0.03460703238937769, 'a': 0.04270213311537414, '_': 0.0651200378223312, 'b': 0.03230547943010081, 'y': 0.03340529192108595, 'j': 0.029899113739442602, 'n': 0.04099748385339039, 'u': 0.034592849015177014, 's': 0.0398483901469622, 't': 0.043861323461063834, 'v': 0.03149197878103114, 'o': 0.04241846563136048, 'l': 0.03628235331826851, 'i': 0.04083353366347741, 'c': 0.03334663525490007, 'h': 0.039862813917335774, 'p': 0.03216364568809398, 'r': 0.03946519864737087, 'w': 0.03354568328605542, 'd': 0.036500633043255286, 'f': 0.03326922768722855, 'g': 0.03291632610542174, 'x': 0.029960895555876086, 'q': 0.029848390146962195, 'k': 0.030774636601118645, 'z': 0.029671698959885893}\n"
     ]
    }
   ],
   "source": [
    "#inject 20% of English's probability to the mix and reduce the influence of the original dict to 80%\n",
    "eng_dict_20 = drifted_dict(0.2, uniform_dict, english_freq)\n",
    "print(eng_dict_20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities after 20% interpolation by English\n",
    "samples = 124\n",
    "all_letters= list(eng_dict_20.keys())\n",
    "eng_20_probs = list(eng_dict_20.values())\n",
    "naive_text = generate_random_strings(all_letters, eng_20_probs, 10, 124)\n",
    "save_numpy_array(naive_text, name=f\"/Users/yunfeiliang/Documents/EMAI_Courses/Semester_2_RU/Course2_NaCo/Final_Project/string_datasets/negative-selection/NaiveStrings/20per_drift/Eng20drift_strings_{samples}samples.test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 0.06358046059906727, 'm': 0.03217702774171835, 'a': 0.04836722919371124, '_': 0.09320303860762535, 'b': 0.027573921823164572, 'y': 0.02977354680513486, 'j': 0.022761190441848163, 'n': 0.04495793066974374, 'u': 0.03214866099331699, 's': 0.04265974325688735, 't': 0.050685609885090625, 'v': 0.02594692052502524, 'o': 0.047799894225683925, 'l': 0.035527669599499975, 'i': 0.044630030289917785, 'c': 0.02965623347276311, 'h': 0.0426885907976345, 'p': 0.02729025433915092, 'r': 0.041893360257704695, 'w': 0.030054329535073802, 'd': 0.03596422904947353, 'f': 0.029501418337420066, 'g': 0.02879561517380643, 'x': 0.022884754074715127, 'q': 0.022659743256887348, 'k': 0.02451223616520025, 'z': 0.022306360882734744}\n"
     ]
    }
   ],
   "source": [
    "# probabilities after 40% interpolation by English\n",
    "samples = 124\n",
    "drift_val_per = 40\n",
    "drift_val = 0.4\n",
    "eng_dict_40 = drifted_dict(drift_val, uniform_dict, english_freq)\n",
    "print(eng_dict_40)\n",
    "all_letters= list(eng_dict_40.keys())\n",
    "eng_probs = list(eng_dict_40.values())\n",
    "naive_text = generate_random_strings(all_letters, eng_probs, 10, 124)\n",
    "save_numpy_array(naive_text, name=f\"/Users/yunfeiliang/Documents/EMAI_Courses/Semester_2_RU/Course2_NaCo/Final_Project/string_datasets/negative-selection/NaiveStrings/{drift_val_per}per_drift/Eng{drift_val_per}drift_strings_{samples}samples.test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 0.0901238841610975, 'm': 0.027317018446399666, 'a': 0.059697421350385436, '_': 0.1493690401782137, 'b': 0.01811080660929211, 'y': 0.022510056573232684, 'j': 0.008485343846659292, 'n': 0.05287882430245044, 'u': 0.027260284949596935, 's': 0.04828244947673767, 't': 0.06433418273314422, 'v': 0.014856804013013444, 'o': 0.058562751414330816, 'l': 0.034018302161962916, 'i': 0.05222302354279853, 'c': 0.022275429908489186, 'h': 0.04834014455823197, 'p': 0.017543471641264803, 'r': 0.046749683478372356, 'w': 0.023071622033110566, 'd': 0.034891421061910025, 'f': 0.021965799637803096, 'g': 0.020554193310575827, 'x': 0.008732471112393222, 'q': 0.00828244947673766, 'k': 0.01198743529336346, 'z': 0.007575684728432454}\n"
     ]
    }
   ],
   "source": [
    "# probabilities after 60% interpolation by English\n",
    "samples = 124\n",
    "drift_val_per = 80\n",
    "drift_val = drift_val_per/100\n",
    "eng_dict_80 = drifted_dict(drift_val, uniform_dict, english_freq)\n",
    "print(eng_dict_80)\n",
    "all_letters= list(eng_dict_80.keys())\n",
    "eng_probs = list(eng_dict_80.values())\n",
    "naive_text = generate_random_strings(all_letters, eng_probs, 10, 124)\n",
    "save_numpy_array(naive_text, name=f\"/Users/yunfeiliang/Documents/EMAI_Courses/Semester_2_RU/Course2_NaCo/Final_Project/string_datasets/negative-selection/NaiveStrings/{drift_val_per}per_drift/Eng{drift_val_per}drift_strings_{samples}samples.test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Naive Malicious URL Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '/Users/yunfeiliang/Documents/EMAI_Courses/Semester_2_RU/Course2_NaCo/Final_Project/string_datasets/negative-selection/AnomalousURL_Datasets/OriginalSourceFiles/malicious_phish.csv'\n",
    "\n",
    "df = pd.read_csv(source)\n",
    "benign_df = df[df['type'] == 'benign']\n",
    "phishing_df = df[df['type'] == 'phishing']\n",
    "benign_df['url'].to_csv('string_datasets/negative-selection/AnomalousURL_Datasets/OriginalSourceFiles/benign_urls.txt', index=False, header=False)\n",
    "phishing_df['url'].to_csv('string_datasets/negative-selection/AnomalousURL_Datasets/OriginalSourceFiles/benign_urls.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of benign chars: 231\n",
      "number of phising chars: 231\n"
     ]
    }
   ],
   "source": [
    "benign_path = 'string_datasets/negative-selection/AnomalousURL_Datasets/OriginalSourceFiles/benign_urls.txt'\n",
    "phising_path = 'string_datasets/negative-selection/AnomalousURL_Datasets/OriginalSourceFiles/benign_urls.txt'\n",
    "\n",
    "with open(benign_path, 'r', encoding='utf-8') as f:\n",
    "    benign_text = f.read()\n",
    "\n",
    "benign_url_counts = Counter(benign_text)\n",
    "benign_total = sum(benign_url_counts.values())\n",
    "\n",
    "benign_url_freqs = {char: count/benign_total for char, count in benign_url_counts.items()}\n",
    "\n",
    "with open(phising_path, 'r', encoding='utf-8') as f:\n",
    "    phish_text = f.read()\n",
    "\n",
    "phishing_url_counts = Counter(phish_text)\n",
    "phish_total = sum(phishing_url_counts.values())\n",
    "\n",
    "# 3. Compute frequencies\n",
    "phishing_url_freqs = {char: count/phish_total for char, count in phishing_url_counts.items()}\n",
    "\n",
    "print(\"number of benign chars:\", len(benign_url_freqs.keys()))\n",
    "print(\"number of phising chars:\", len(phishing_url_freqs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the keys are in the same order\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "if (list(phishing_url_freqs.keys()) == list(benign_url_freqs.keys())):\n",
    "    print(\"the keys are in the same order\")\n",
    "print(sum(phishing_url_freqs.values()))\n",
    "print(sum(benign_url_freqs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p': 0.028484706981088022, '9': 0.010973616623861778, '.': 0.04074247023114639, 's': 0.03829091758113472, 'e': 0.06105533504552883, 'c': 0.052883492878823254, 'u': 0.01774457156198926, 'r': 0.03000233481204763, 'h': 0.014475834695307027, 'o': 0.04039224842400187, 't': 0.027550782162035957, 'i': 0.041793135652579964, 'n': 0.03642306794303059, 'g': 0.015176278309596078, 'd': 0.04120943264067243, 'm': 0.027550782162035957, '/': 0.03759047396684567, '@': 0.00011674060238150829, 'a': 0.05580200793836096, '-': 0.00957272939528368, 'l': 0.025566191921550314, 'v': 0.004319402288115806, '\\n': 0.011674060238150829, 'w': 0.024982488909642775, 'b': 0.022180714452486573, 'L': 0.0018678496381041327, 'f': 0.024048564090590707, '3': 0.025449451319168807, '2': 0.014826056502451553, '5': 0.019028718188185852, '0': 0.021597011440579034, '8': 0.024398785897735233, '4': 0.017978052766752278, '6': 0.017978052766752278, '1': 0.020896567826289982, '7': 0.012024282045295353, 'j': 0.0030352556619192153, 'k': 0.006537473733364464, 'y': 0.00887228578099463, 'T': 0.0010506654214335746, 'S': 0.003268736866682232, '_': 0.004202661685734298, 'M': 0.001984590240485641, 'x': 0.003151996264300724, '?': 0.002451552650011674, '=': 0.005486808311930889, 'q': 0.005603548914312398, 'V': 0.0018678496381041327, 'J': 0.0012841466261965912, 'P': 0.0023348120476301658, 'O': 0.0022180714452486577, 'K': 0.0009339248190520663, 'W': 0.0015176278309596077, 'X': 0.0010506654214335746, 'N': 0.0022180714452486577, 'H': 0.0022180714452486577, 'U': 0.0015176278309596077, 'Q': 0.0010506654214335746, 'z': 0.0022180714452486577, 'E': 0.0007004436142890498, 'R': 0.0012841466261965912, 'Y': 0.0005837030119075414, 'I': 0.0015176278309596077, 'B': 0.0010506654214335746, 'A': 0.000817184216670558, '&': 0.003151996264300724, 'F': 0.0014008872285780996, 'D': 0.0018678496381041327, 'G': 0.000817184216670558, 'C': 0.0012841466261965912, 'Z': 0.0010506654214335746, ';': 0.0007004436142890498, \"'\": 0.00046696240952603317, '\\\\': 0.00023348120476301658, '%': 0.00023348120476301658, ':': 0.00011674060238150829}\n",
      "76\n",
      "0.06105533504552883\n",
      "Top 10 values [0.06105533504552883, 0.05580200793836096, 0.052883492878823254, 0.041793135652579964, 0.04120943264067243, 0.04074247023114639, 0.04039224842400187, 0.03829091758113472, 0.03759047396684567, 0.03642306794303059]\n"
     ]
    }
   ],
   "source": [
    "print(benign_url_freqs)\n",
    "print(len(benign_url_freqs.keys()))\n",
    "print(max(benign_url_freqs.values()))\n",
    "print(\"Top 10 values\", sorted(benign_url_freqs.values(), reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test file\n",
    "features = 10\n",
    "samples = 100\n",
    "file_type = \"test\"\n",
    "url_type = \"benign\"\n",
    "benign_url_chars = list(benign_url_freqs.keys())\n",
    "benign_url_probs = list(benign_url_freqs.values())\n",
    "\n",
    "naive_url_text = generate_random_strings(benign_url_chars, benign_url_probs, features, samples)\n",
    "save_numpy_array(naive_url_text, name=f\"string_datasets/negative-selection/AnomalousURL_Datasets/Naive{url_type}URLStrings{samples}Samples.{file_type}\")\n",
    "# print(naive_url_text.shape)\n",
    "# print(naive_url_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train file\n",
    "features = 10\n",
    "samples = 500\n",
    "file_type = \"train\"\n",
    "url_type = \"benign\"\n",
    "benign_url_chars = list(benign_url_freqs.keys())\n",
    "benign_url_probs = list(benign_url_freqs.values())\n",
    "\n",
    "naive_url_text = generate_random_strings(benign_url_chars, benign_url_probs, features, samples)\n",
    "save_numpy_array(naive_url_text, name=f\"string_datasets/negative-selection/AnomalousURL_Datasets/Naive{url_type}URLStrings{samples}Samples.{file_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phishint test file\n",
    "features = 10\n",
    "samples = 100\n",
    "file_type = \"test\"\n",
    "url_type = \"phishing\"\n",
    "phish_url_chars = list(benign_url_freqs.keys())\n",
    "phish_url_probs = list(benign_url_freqs.values())\n",
    "\n",
    "naive_url_text = generate_random_strings(phish_url_chars, phish_url_probs, features, samples)\n",
    "save_numpy_array(naive_url_text, name=f\"string_datasets/negative-selection/AnomalousURL_Datasets/Naive{url_type}URLStrings{samples}Samples.{file_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03669927626990562, 0.023624012120269936, 0.009381809158905428, 0.04115054691073549, 0.05622182460662271, 0.04687226132390874, 0.023980586709587134, 0.033035757998553114, 0.04221179123383673, 0.05911359396779288, 0.04661767893039713, 0.018501113713852213, 0.042571510781572605, 0.009730740304977243, 0.0024322472456417108, 0.008058737095049557, 0.0333301499923011, 0.022490672611140617, 0.04911425839002082, 0.017042649199592436, 0.016032082497592092, 0.06774204615182516, 0.016454382736899545, 0.0038766191614178136, 0.013560224799715189, 0.0016067155655400337, 0.01204566855872236, 0.040840151457692486, 0.012696674951387776, 0.03133481320925359, 0.0015991915510953593, 0.011725400324820737, 0.011929583764433095, 0.00762696217088585, 0.008325740083834913, 0.006513208985072828, 0.003080984491093653, 0.0054990196094509775, 0.006709231456636731, 0.007677998078388881, 0.014284600285616242, 0.0010928930552891612, 0.0027668469356284297, 0.0016311188504849307, 0.007473695209975812, 0.009764817322779578, 0.0036603734129338327, 0.0037530103526838736, 0.002516404740541404, 0.0013255801686707642, 0.0014864109536257132, 0.0014557177518434692, 0.00155110155400982, 0.002794912703795073, 0.0020010296558813133, 0.0018254693188389062, 0.0030617564541794845, 0.00014769371687694562, 0.00014717619207387276, 0.0008812652204325998, 0.002682331154326609, 0.0008846092268524552, 0.0010628766167109357, 0.0011212773002576956, 0.0015016978401164805, 0.0010040380275615757, 0.0021528236615826054, 0.001698954409287711, 0.0006941999089218808, 0.0007875932310764131, 0.0007020424001684464, 0.009169305512843668, 0.0005344837927735505, 0.0008192816728645663, 0.00031441632266687773, 0.0001662051809868593, 1.7915320106368068e-06, 0.0005160917574643459, 5.863964114817814e-05, 5.8560021947705394e-05, 0.0007508887796584767, 2.68715801595521e-05, 1.9914800118186743e-07, 1.9914800118186743e-07, 7.88240084680195e-06, 3.5838640212736135e-07, 1.1385645667602817e-05, 8.320306449402058e-06, 4.778152028364818e-07, 1.0351496061457106e-06, 5.334586431674047e-06, 1.3536264080366985e-06, 1.3536264080366985e-06, 1.9914800118186743e-07, 9.95340005909337e-07, 6.768632040183492e-07, 6.370536037819757e-07, 3.9819600236373486e-07, 2.787672016546144e-07, 1.5933840094549393e-07, 2.389576014182409e-07, 4.380056026001083e-07, 7.971920047274695e-08, 1.3934360082730718e-06, 1.1952880070912045e-07, 2.389576014182409e-07, 7.971920047274695e-08, 1.1952880070912045e-07, 1.1952880070912045e-07, 2.389576014182409e-07, 1.1952880070912045e-07, 3.990960023637348e-08, 1.1952880070912045e-07, 1.5933840094549393e-07, 1.5933840094549393e-07, 7.971920047274695e-08, 3.990960023637348e-08, 1.5933840094549393e-07, 3.990960023637348e-08, 1.1952880070912045e-07, 2.389576014182409e-07, 7.971920047274695e-08, 1.9914800118186743e-07, 1.5933840094549393e-07, 7.971920047274695e-08, 1.1952880070912045e-07, 7.971920047274695e-08, 0.00049606752854545, 3.990960023637348e-08, 1.1952880070912045e-07, 1.9914800118186743e-07, 5.574344033092288e-07, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.5838640212736135e-07, 1.5933840094549393e-07, 3.990960023637348e-08, 9.992309659329744e-06, 5.931730435219649e-06, 1.759594330447708e-05, 2.7946439365934185e-05, 7.971920047274695e-08, 1.5933840094549393e-07, 7.971920047274695e-08, 3.990960023637348e-08, 7.971920047274695e-08, 1.1952880070912045e-07, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 1.1952880070912045e-07, 1.1952880070912045e-07, 1.1952880070912045e-07, 3.990960023637348e-08, 7.971920047274695e-08, 7.971920047274695e-08, 7.971920047274695e-08, 3.990960023637348e-08, 1.1952880070912045e-07, 7.971920047274695e-08, 7.971920047274695e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 1.1952880070912045e-07, 3.990960023637348e-08, 1.1952880070912045e-07, 7.971920047274695e-08, 3.990960023637348e-08, 1.1952880070912045e-07, 3.990960023637348e-08, 1.5933840094549393e-07, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 7.971920047274695e-08, 1.1952880070912045e-07, 7.971920047274695e-08, 7.971920047274695e-08, 1.1952880070912045e-07, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 7.971920047274695e-08, 7.971920047274695e-08, 1.1952880070912045e-07, 7.971920047274695e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08]\n",
      "[0.03669927626990562, 0.023624012120269936, 0.009381809158905428, 0.04115054691073549, 0.05622182460662271, 0.04687226132390874, 0.023980586709587134, 0.033035757998553114, 0.04221179123383673, 0.05911359396779288, 0.04661767893039713, 0.018501113713852213, 0.042571510781572605, 0.009730740304977243, 0.0024322472456417108, 0.008058737095049557, 0.0333301499923011, 0.022490672611140617, 0.04911425839002082, 0.017042649199592436, 0.016032082497592092, 0.06774204615182516, 0.016454382736899545, 0.0038766191614178136, 0.013560224799715189, 0.0016067155655400337, 0.01204566855872236, 0.040840151457692486, 0.012696674951387776, 0.03133481320925359, 0.0015991915510953593, 0.011725400324820737, 0.011929583764433095, 0.00762696217088585, 0.008325740083834913, 0.006513208985072828, 0.003080984491093653, 0.0054990196094509775, 0.006709231456636731, 0.007677998078388881, 0.014284600285616242, 0.0010928930552891612, 0.0027668469356284297, 0.0016311188504849307, 0.007473695209975812, 0.009764817322779578, 0.0036603734129338327, 0.0037530103526838736, 0.002516404740541404, 0.0013255801686707642, 0.0014864109536257132, 0.0014557177518434692, 0.00155110155400982, 0.002794912703795073, 0.0020010296558813133, 0.0018254693188389062, 0.0030617564541794845, 0.00014769371687694562, 0.00014717619207387276, 0.0008812652204325998, 0.002682331154326609, 0.0008846092268524552, 0.0010628766167109357, 0.0011212773002576956, 0.0015016978401164805, 0.0010040380275615757, 0.0021528236615826054, 0.001698954409287711, 0.0006941999089218808, 0.0007875932310764131, 0.0007020424001684464, 0.009169305512843668, 0.0005344837927735505, 0.0008192816728645663, 0.00031441632266687773, 0.0001662051809868593, 1.7915320106368068e-06, 0.0005160917574643459, 5.863964114817814e-05, 5.8560021947705394e-05, 0.0007508887796584767, 2.68715801595521e-05, 1.9914800118186743e-07, 1.9914800118186743e-07, 7.88240084680195e-06, 3.5838640212736135e-07, 1.1385645667602817e-05, 8.320306449402058e-06, 4.778152028364818e-07, 1.0351496061457106e-06, 5.334586431674047e-06, 1.3536264080366985e-06, 1.3536264080366985e-06, 1.9914800118186743e-07, 9.95340005909337e-07, 6.768632040183492e-07, 6.370536037819757e-07, 3.9819600236373486e-07, 2.787672016546144e-07, 1.5933840094549393e-07, 2.389576014182409e-07, 4.380056026001083e-07, 7.971920047274695e-08, 1.3934360082730718e-06, 1.1952880070912045e-07, 2.389576014182409e-07, 7.971920047274695e-08, 1.1952880070912045e-07, 1.1952880070912045e-07, 2.389576014182409e-07, 1.1952880070912045e-07, 3.990960023637348e-08, 1.1952880070912045e-07, 1.5933840094549393e-07, 1.5933840094549393e-07, 7.971920047274695e-08, 3.990960023637348e-08, 1.5933840094549393e-07, 3.990960023637348e-08, 1.1952880070912045e-07, 2.389576014182409e-07, 7.971920047274695e-08, 1.9914800118186743e-07, 1.5933840094549393e-07, 7.971920047274695e-08, 1.1952880070912045e-07, 7.971920047274695e-08, 0.00049606752854545, 3.990960023637348e-08, 1.1952880070912045e-07, 1.9914800118186743e-07, 5.574344033092288e-07, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.5838640212736135e-07, 1.5933840094549393e-07, 3.990960023637348e-08, 9.992309659329744e-06, 5.931730435219649e-06, 1.759594330447708e-05, 2.7946439365934185e-05, 7.971920047274695e-08, 1.5933840094549393e-07, 7.971920047274695e-08, 3.990960023637348e-08, 7.971920047274695e-08, 1.1952880070912045e-07, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 1.1952880070912045e-07, 1.1952880070912045e-07, 1.1952880070912045e-07, 3.990960023637348e-08, 7.971920047274695e-08, 7.971920047274695e-08, 7.971920047274695e-08, 3.990960023637348e-08, 1.1952880070912045e-07, 7.971920047274695e-08, 7.971920047274695e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 1.1952880070912045e-07, 3.990960023637348e-08, 1.1952880070912045e-07, 7.971920047274695e-08, 3.990960023637348e-08, 1.1952880070912045e-07, 3.990960023637348e-08, 1.5933840094549393e-07, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 7.971920047274695e-08, 1.1952880070912045e-07, 7.971920047274695e-08, 7.971920047274695e-08, 1.1952880070912045e-07, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 7.971920047274695e-08, 7.971920047274695e-08, 1.1952880070912045e-07, 7.971920047274695e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 7.971920047274695e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08, 3.990960023637348e-08]\n"
     ]
    }
   ],
   "source": [
    "#KL Divergence of the 2 set of probabilities\n",
    "eps = 1e-10\n",
    "eps = 1e-10\n",
    "benign_probs_safe = [prob + eps for prob in benign_url_probs]\n",
    "phishing_probs_safe = [prob + eps for prob in phish_url_probs]\n",
    "\n",
    "print(benign_probs_safe)\n",
    "print(phishing_probs_safe)\n",
    "\n",
    "# kl_div = entropy(benign_probs_safe, phishing_probs_safe)\n",
    "# print(f\"KL Divergence: {kl_div}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['m', 'p', '3', 'r', 'a', 'i', 'd', '.', 'c', 'o', '/', 'u', 's', 'k', 'z', '_', 'l', 'h', 't', '\\n', 'b', 'e', 'g', 'x', '1', ':', 'f', 'n', 'w', '-', 'q', '2', 'y', '4', '5', '7', '?', '=', '6', '9', '0', 'O', 'C', 'P', 'v', '8', 'A', 'D', '&', 'N', 'L', 'R', 'T', 'j', 'F', 'M', 'B', '(', ')', 'J', 'E', 'U', 'G', 'H', 'I', 'W', 'S', '+', 'Y', 'K', 'Q', '%', 'Z', 'V', \"'\", '~', '$', 'X', '[', ']', ';', '@', '“', '”', 'é', 'â', '!', '|', 'É', 'è', '*', '{', '}', 'î', '–', 'ç', '’', 'Ã', '©', '‚', 'š', 'ô', '—', '^', 'û', 'ó', 'ê', 'Ö', 'ö', 'ü', 'ƒ', 'Æ', 'Â', '§', 'á', '»', '•', '®', '™', '°', 'ñ', '¿', 'à', '´', 'ä', 'ë', '¨', '\\\\', 'ž', 'í', '`', '>', 'ß', 'ã', '…', '‰', '<', 'Î', '€', ' ', '#', '\"', ',', '\\x99', '\\xa0', '¼', 'Ò', '\\x8e', '¥', '\\x1d', '\\x02', 'å', '£', 'Å', 'Í', 'Ï', '¸', '\\x0f', '\\x7f', 'ÿ', '\\x9a', '\\x16', '\\x91', '\\x19', '\\x01', 'Ù', '\\x9b', '\\x8b', '¡', '×', '\\t', 'Ý', '\\x83', '\\x88', '\\x86', '\\x06', '\\x18', 'Ì', '¶', '\\x08', 'Ë', '±', '\\x98', '\\x8c', 'º', '÷', '\\x11', 'ú', 'ò', 'Õ', '¢', 'Á', '\\x1c', '¬', '\\x1a', '\\x9c', '\\x96', 'æ', '\\x89', 'Ô', 'ï', '\\x97', '\\x82', '\\x1f', 'ø', 'À', '\\x13', '\\x14', 'Ñ', '\\x04', '½', 'õ', '\\x95', '\\x8f', '\\x94', 'Ø', '³', '²', '\\x8a', '\\x1e', 'ì', 'ð', '\\xad', '\\x9e', '\\x85', '¦', 'Ç', '·', 'È', '\\x05', 'ª'])\n"
     ]
    }
   ],
   "source": [
    "print(benign_url_freqs[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 10\n",
    "samples = 500\n",
    "benign_url_chars = list(benign_url_freqs.keys())\n",
    "benign_url_probs = list(benign_url_freqs.values())\n",
    "\n",
    "naive_url_text = generate_random_strings(benign_url_chars, benign_url_probs, features, samples)\n",
    "save_numpy_array(naive_url_text, name=f\"string_datasets/negative-selection/AnomalousURL_Datasets/NaiveSyntheticURLStrings{samples}Samples.train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
